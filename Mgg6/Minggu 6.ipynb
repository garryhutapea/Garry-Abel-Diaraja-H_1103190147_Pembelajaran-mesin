{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Minggu 6.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfW4Otf7mpuXo2E3ILfVnD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Bz4Om2TahgeJ"},"source":["!pip install d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"06ePkS1RaUKN"},"source":["%matplotlib inline\n","import tensorflow as tf\n","from d2l import tensorflow as d2l\n","\n","\n","T = 1000  # Generate a total of 1000 points\n","time = tf.range(1, T + 1, dtype=tf.float32)\n","x = tf.sin(0.01 * time) + tf.random.normal([T], 0, 0.2)\n","d2l.plot(time, [x], 'time', 'x', xlim=[1, 1000], figsize=(6, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uOUfihhRirq8"},"source":["tau = 4\n","features = tf.Variable(tf.zeros((T - tau, tau)))\n","for i in range(tau):\n","    features[:, i].assign(x[i:T - tau + i])\n","labels = tf.reshape(x[tau:], (-1, 1))\n","\n","batch_size, n_train = 16, 600\n","# Only the first `n_train` examples are used for training\n","train_iter = d2l.load_array((features[:n_train], labels[:n_train]),\n","                            batch_size, is_train=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fytjUocAixCI"},"source":["# Vanilla MLP architecture\n","def get_net():\n","    net = tf.keras.Sequential([\n","        tf.keras.layers.Dense(10, activation='relu'),\n","        tf.keras.layers.Dense(1)])\n","    return net\n","\n","# Least mean squares loss\n","# Note: L2 Loss = 1/2 * MSE Loss. TensorFlow has MSE Loss that is slightly\n","# different from MXNet's L2Loss by a factor of 2. Hence we halve the loss\n","# value to get L2Loss in TF\n","loss = tf.keras.losses.MeanSquaredError()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9qkM5viDiyJW"},"source":["def train(net, train_iter, loss, epochs, lr):\n","    trainer = tf.keras.optimizers.Adam()\n","    for epoch in range(epochs):\n","        for X, y in train_iter:\n","            with tf.GradientTape() as g:\n","                out = net(X)\n","                l = loss(y, out) / 2\n","                params = net.trainable_variables\n","                grads = g.gradient(l, params)\n","            trainer.apply_gradients(zip(grads, params))\n","        print(f'epoch {epoch + 1}, '\n","              f'loss: {d2l.evaluate_loss(net, train_iter, loss):f}')\n","\n","net = get_net()\n","train(net, train_iter, loss, 5, 0.01)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MuYK22Zji3Ra"},"source":["onestep_preds = net(features)\n","d2l.plot([time, time[tau:]], [x.numpy(), onestep_preds.numpy()], 'time', 'x',\n","         legend=['data', '1-step preds'], xlim=[1, 1000], figsize=(6, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OVczzmgei55y"},"source":["multistep_preds = tf.Variable(tf.zeros(T))\n","multistep_preds[:n_train + tau].assign(x[:n_train + tau])\n","for i in range(n_train + tau, T):\n","    multistep_preds[i].assign(\n","        tf.reshape(net(tf.reshape(multistep_preds[i - tau:i], (1, -1))), ()))\n","\n","d2l.plot([time, time[tau:], time[n_train + tau:]], [\n","    x.numpy(),\n","    onestep_preds.numpy(), multistep_preds[n_train + tau:].numpy()], 'time',\n","         'x', legend=['data', '1-step preds',\n","                      'multistep preds'], xlim=[1, 1000], figsize=(6, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJjNQtwji9J0"},"source":["max_steps = 64\n","\n","features = tf.Variable(tf.zeros((T - tau - max_steps + 1, tau + max_steps)))\n","# Column `i` (`i` < `tau`) are observations from `x` for time steps from\n","# `i + 1` to `i + T - tau - max_steps + 1`\n","for i in range(tau):\n","    features[:, i].assign(x[i:i + T - tau - max_steps + 1].numpy())\n","\n","# Column `i` (`i` >= `tau`) are the (`i - tau + 1`)-step-ahead predictions for\n","# time steps from `i + 1` to `i + T - tau - max_steps + 1`\n","for i in range(tau, tau + max_steps):\n","    features[:, i].assign(tf.reshape(net((features[:, i - tau:i])), -1))\n","\n","steps = (1, 4, 16, 64)\n","d2l.plot([time[tau + i - 1:T - max_steps + i] for i in steps],\n","         [features[:, (tau + i - 1)].numpy() for i in steps], 'time', 'x',\n","         legend=[f'{i}-step preds'\n","                 for i in steps], xlim=[5, 1000], figsize=(6, 3))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RGzkW3EjjFKM"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yB46woHsjFVb"},"source":["import collections\n","import re\n","from d2l import tensorflow as d2l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0PPaksDjHBU"},"source":["#reading dataset\n","#@save\n","d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt',\n","                                '090b5e7e70c295757f55df93cb0a180b9691891a')\n","\n","def read_time_machine():  #@save\n","    \"\"\"Load the time machine dataset into a list of text lines.\"\"\"\n","    with open(d2l.download('time_machine'), 'r') as f:\n","        lines = f.readlines()\n","    return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n","\n","lines = read_time_machine()\n","print(f'# text lines: {len(lines)}')\n","print(lines[0])\n","print(lines[10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QpPjRh0jK4E"},"source":["#Tokenization\n","def tokenize(lines, token='word'):  #@save\n","    \"\"\"Split text lines into word or character tokens.\"\"\"\n","    if token == 'word':\n","        return [line.split() for line in lines]\n","    elif token == 'char':\n","        return [list(line) for line in lines]\n","    else:\n","        print('ERROR: unknown token type: ' + token)\n","\n","tokens = tokenize(lines)\n","for i in range(11):\n","    print(tokens[i])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrMcBTdRjOJs"},"source":["#Vocabulary\n","class Vocab:  #@save\n","    \"\"\"Vocabulary for text.\"\"\"\n","    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n","        if tokens is None:\n","            tokens = []\n","        if reserved_tokens is None:\n","            reserved_tokens = []\n","        # Sort according to frequencies\n","        counter = count_corpus(tokens)\n","        self._token_freqs = sorted(counter.items(), key=lambda x: x[1],\n","                                   reverse=True)\n","        # The index for the unknown token is 0\n","        self.idx_to_token = ['<unk>'] + reserved_tokens\n","        self.token_to_idx = {\n","            token: idx for idx, token in enumerate(self.idx_to_token)}\n","        for token, freq in self._token_freqs:\n","            if freq < min_freq:\n","                break\n","            if token not in self.token_to_idx:\n","                self.idx_to_token.append(token)\n","                self.token_to_idx[token] = len(self.idx_to_token) - 1\n","\n","    def __len__(self):\n","        return len(self.idx_to_token)\n","\n","    def __getitem__(self, tokens):\n","        if not isinstance(tokens, (list, tuple)):\n","            return self.token_to_idx.get(tokens, self.unk)\n","        return [self.__getitem__(token) for token in tokens]\n","\n","    def to_tokens(self, indices):\n","        if not isinstance(indices, (list, tuple)):\n","            return self.idx_to_token[indices]\n","        return [self.idx_to_token[index] for index in indices]\n","\n","    @property\n","    def unk(self):  # Index for the unknown token\n","        return 0\n","\n","    @property\n","    def token_freqs(self):  # Index for the unknown token\n","        return self._token_freqs\n","\n","def count_corpus(tokens):  #@save\n","    \"\"\"Count token frequencies.\"\"\"\n","    # Here `tokens` is a 1D list or 2D list\n","    if len(tokens) == 0 or isinstance(tokens[0], list):\n","        # Flatten a list of token lists into a list of tokens\n","        tokens = [token for line in tokens for token in line]\n","    return collections.Counter(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HakguQGHjSSa"},"source":["vocab = Vocab(tokens)\n","print(list(vocab.token_to_idx.items())[:10])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0tuzT80jVKF"},"source":["for i in [0, 10]:\n","    print('words:', tokens[i])\n","    print('indices:', vocab[tokens[i]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RFEOPK5WjZwq"},"source":["def load_corpus_time_machine(max_tokens=-1):  #@save\n","    \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\"\n","    lines = read_time_machine()\n","    tokens = tokenize(lines, 'char')\n","    vocab = Vocab(tokens)\n","    # Since each text line in the time machine dataset is not necessarily a\n","    # sentence or a paragraph, flatten all the text lines into a single list\n","    corpus = [vocab[token] for line in tokens for token in line]\n","    if max_tokens > 0:\n","        corpus = corpus[:max_tokens]\n","    return corpus, vocab\n","\n","corpus, vocab = load_corpus_time_machine()\n","len(corpus), len(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HPMJ1pWXjhNy"},"source":["import random\n","import tensorflow as tf\n","from d2l import tensorflow as d2l\n","\n","tokens = d2l.tokenize(d2l.read_time_machine())\n","# Since each text line is not necessarily a sentence or a paragraph, we\n","# concatenate all text lines\n","corpus = [token for line in tokens for token in line]\n","vocab = d2l.Vocab(corpus)\n","vocab.token_freqs[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xcb6FY9cjif5"},"source":["freqs = [freq for token, freq in vocab.token_freqs]\n","d2l.plot(freqs, xlabel='token: x', ylabel='frequency: n(x)', xscale='log',\n","         yscale='log')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ay5fwNQjlxF"},"source":["bigram_tokens = [pair for pair in zip(corpus[:-1], corpus[1:])]\n","bigram_vocab = d2l.Vocab(bigram_tokens)\n","bigram_vocab.token_freqs[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S0YsNxxYjoZ_"},"source":["trigram_tokens = [\n","    triple for triple in zip(corpus[:-2], corpus[1:-1], corpus[2:])]\n","trigram_vocab = d2l.Vocab(trigram_tokens)\n","trigram_vocab.token_freqs[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UX57HE5djqlY"},"source":["bigram_freqs = [freq for token, freq in bigram_vocab.token_freqs]\n","trigram_freqs = [freq for token, freq in trigram_vocab.token_freqs]\n","d2l.plot([freqs, bigram_freqs, trigram_freqs], xlabel='token: x',\n","         ylabel='frequency: n(x)', xscale='log', yscale='log',\n","         legend=['unigram', 'bigram', 'trigram'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rjl-vAEXjtek"},"source":["#random sampling\n","def seq_data_iter_random(corpus, batch_size, num_steps):  #@save\n","    \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n","    # Start with a random offset (inclusive of `num_steps - 1`) to partition a\n","    # sequence\n","    corpus = corpus[random.randint(0, num_steps - 1):]\n","    # Subtract 1 since we need to account for labels\n","    num_subseqs = (len(corpus) - 1) // num_steps\n","    # The starting indices for subsequences of length `num_steps`\n","    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n","    # In random sampling, the subsequences from two adjacent random\n","    # minibatches during iteration are not necessarily adjacent on the\n","    # original sequence\n","    random.shuffle(initial_indices)\n","\n","    def data(pos):\n","        # Return a sequence of length `num_steps` starting from `pos`\n","        return corpus[pos:pos + num_steps]\n","\n","    num_batches = num_subseqs // batch_size\n","    for i in range(0, batch_size * num_batches, batch_size):\n","        # Here, `initial_indices` contains randomized starting indices for\n","        # subsequences\n","        initial_indices_per_batch = initial_indices[i:i + batch_size]\n","        X = [data(j) for j in initial_indices_per_batch]\n","        Y = [data(j + 1) for j in initial_indices_per_batch]\n","        yield tf.constant(X), tf.constant(Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIPExf5Qjw8C"},"source":["my_seq = list(range(35))\n","for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):\n","    print('X: ', X, '\\nY:', Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwivSrgjj2ED"},"source":["def seq_data_iter_sequential(corpus, batch_size, num_steps):  #@save\n","    \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\"\n","    # Start with a random offset to partition a sequence\n","    offset = random.randint(0, num_steps)\n","    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n","    Xs = tf.constant(corpus[offset:offset + num_tokens])\n","    Ys = tf.constant(corpus[offset + 1:offset + 1 + num_tokens])\n","    Xs = tf.reshape(Xs, (batch_size, -1))\n","    Ys = tf.reshape(Ys, (batch_size, -1))\n","    num_batches = Xs.shape[1] // num_steps\n","    for i in range(0, num_batches * num_steps, num_steps):\n","        X = Xs[:, i:i + num_steps]\n","        Y = Ys[:, i:i + num_steps]\n","        yield X, Y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hm363UOgj5K3"},"source":["for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):\n","    print('X: ', X, '\\nY:', Y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sjFHRZL4j7-a"},"source":["class SeqDataLoader:  #@save\n","    \"\"\"An iterator to load sequence data.\"\"\"\n","    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n","        if use_random_iter:\n","            self.data_iter_fn = d2l.seq_data_iter_random\n","        else:\n","            self.data_iter_fn = d2l.seq_data_iter_sequential\n","        self.corpus, self.vocab = d2l.load_corpus_time_machine(max_tokens)\n","        self.batch_size, self.num_steps = batch_size, num_steps\n","\n","    def __iter__(self):\n","        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Zi0h52Cj_sP"},"source":["def load_data_time_machine(batch_size, num_steps,  #@save\n","                           use_random_iter=False, max_tokens=10000):\n","    \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n","    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter,\n","                              max_tokens)\n","    return data_iter, data_iter.vocab"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TVFPSPakCs0"},"source":["import tensorflow as tf\n","from d2l import tensorflow as d2l\n","\n","X, W_xh = tf.random.normal((3, 1), 0, 1), tf.random.normal((1, 4), 0, 1)\n","H, W_hh = tf.random.normal((3, 4), 0, 1), tf.random.normal((4, 4), 0, 1)\n","tf.matmul(X, W_xh) + tf.matmul(H, W_hh)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"taqzWH_SkFy4"},"source":["tf.matmul(tf.concat((X, H), 1), tf.concat((W_xh, W_hh), 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fG7DoINJkJqb"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FPoa4dE2kJrT"},"source":["%matplotlib inline\n","import math\n","import tensorflow as tf\n","from d2l import tensorflow as d2l\n","\n","\n","batch_size, num_steps = 32, 35\n","train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z33QXpTMkLTf"},"source":["tf.one_hot(tf.constant([0, 2]), len(vocab))\n","X = tf.reshape(tf.range(10), (2, 5))\n","tf.one_hot(tf.transpose(X), 28).shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9fk_TbvLkSoc"},"source":["def get_params(vocab_size, num_hiddens):\n","    num_inputs = num_outputs = vocab_size\n","\n","    def normal(shape):\n","        return tf.random.normal(shape=shape, stddev=0.01, mean=0,\n","                                dtype=tf.float32)\n","\n","    # Hidden layer parameters\n","    W_xh = tf.Variable(normal((num_inputs, num_hiddens)), dtype=tf.float32)\n","    W_hh = tf.Variable(normal((num_hiddens, num_hiddens)), dtype=tf.float32)\n","    b_h = tf.Variable(tf.zeros(num_hiddens), dtype=tf.float32)\n","    # Output layer parameters\n","    W_hq = tf.Variable(normal((num_hiddens, num_outputs)), dtype=tf.float32)\n","    b_q = tf.Variable(tf.zeros(num_outputs), dtype=tf.float32)\n","    params = [W_xh, W_hh, b_h, W_hq, b_q]\n","    return params"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IcOa16-akTvC"},"source":["def init_rnn_state(batch_size, num_hiddens):\n","    return (tf.zeros((batch_size, num_hiddens)),)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLB6pY2LkWjy"},"source":["def rnn(inputs, state, params):\n","    # Here `inputs` shape: (`num_steps`, `batch_size`, `vocab_size`)\n","    W_xh, W_hh, b_h, W_hq, b_q = params\n","    H, = state\n","    outputs = []\n","    # Shape of `X`: (`batch_size`, `vocab_size`)\n","    for X in inputs:\n","        X = tf.reshape(X, [-1, W_xh.shape[0]])\n","        H = tf.tanh(tf.matmul(X, W_xh) + tf.matmul(H, W_hh) + b_h)\n","        Y = tf.matmul(H, W_hq) + b_q\n","        outputs.append(Y)\n","    return tf.concat(outputs, axis=0), (H,)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3SSsjNWkaYY"},"source":["class RNNModelScratch:  #@save\n","    \"\"\"A RNN Model implemented from scratch.\"\"\"\n","    def __init__(self, vocab_size, num_hiddens, init_state, forward_fn,\n","                 get_params):\n","        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n","        self.init_state, self.forward_fn = init_state, forward_fn\n","        self.trainable_variables = get_params(vocab_size, num_hiddens)\n","\n","    def __call__(self, X, state):\n","        X = tf.one_hot(tf.transpose(X), self.vocab_size)\n","        X = tf.cast(X, tf.float32)\n","        return self.forward_fn(X, state, self.trainable_variables)\n","\n","    def begin_state(self, batch_size, *args, **kwargs):\n","        return self.init_state(batch_size, self.num_hiddens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Rjc8QVskcn4"},"source":["# defining tensorflow training strategy\n","device_name = d2l.try_gpu()._device_name\n","strategy = tf.distribute.OneDeviceStrategy(device_name)\n","\n","num_hiddens = 512\n","with strategy.scope():\n","    net = RNNModelScratch(len(vocab), num_hiddens, init_rnn_state, rnn,\n","                          get_params)\n","state = net.begin_state(X.shape[0])\n","Y, new_state = net(X, state)\n","Y.shape, len(new_state), new_state[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxdc3eg1kfPO"},"source":["#prediction\n","def predict_ch8(prefix, num_preds, net, vocab):  #@save\n","    \"\"\"Generate new characters following the `prefix`.\"\"\"\n","    state = net.begin_state(batch_size=1, dtype=tf.float32)\n","    outputs = [vocab[prefix[0]]]\n","    get_input = lambda: tf.reshape(tf.constant([outputs[-1]]), (1, 1)).numpy()\n","    for y in prefix[1:]:  # Warm-up period\n","        _, state = net(get_input(), state)\n","        outputs.append(vocab[y])\n","    for _ in range(num_preds):  # Predict `num_preds` steps\n","        y, state = net(get_input(), state)\n","        outputs.append(int(y.numpy().argmax(axis=1).reshape(1)))\n","    return ''.join([vocab.idx_to_token[i] for i in outputs])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tFc-Ik8Vkhq7"},"source":["predict_ch8('time traveller ', 10, net, vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRBynFtRkkYb"},"source":["def grad_clipping(grads, theta):  #@save\n","    \"\"\"Clip the gradient.\"\"\"\n","    theta = tf.constant(theta, dtype=tf.float32)\n","    new_grad = []\n","    for grad in grads:\n","        if isinstance(grad, tf.IndexedSlices):\n","            new_grad.append(tf.convert_to_tensor(grad))\n","        else:\n","            new_grad.append(grad)\n","    norm = tf.math.sqrt(\n","        sum((tf.reduce_sum(grad**2)).numpy() for grad in new_grad))\n","    norm = tf.cast(norm, tf.float32)\n","    if tf.greater(norm, theta):\n","        for i, grad in enumerate(new_grad):\n","            new_grad[i] = grad * theta / norm\n","    else:\n","        new_grad = new_grad\n","    return new_grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"57p8YBXmknhQ"},"source":["#training\n","#@save\n","def train_epoch_ch8(net, train_iter, loss, updater, use_random_iter):\n","    \"\"\"Train a model within one epoch (defined in Chapter 8).\"\"\"\n","    state, timer = None, d2l.Timer()\n","    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n","    for X, Y in train_iter:\n","        if state is None or use_random_iter:\n","            # Initialize `state` when either it is the first iteration or\n","            # using random sampling\n","            state = net.begin_state(batch_size=X.shape[0], dtype=tf.float32)\n","        with tf.GradientTape(persistent=True) as g:\n","            y_hat, state = net(X, state)\n","            y = tf.reshape(tf.transpose(Y), (-1))\n","            l = loss(y, y_hat)\n","        params = net.trainable_variables\n","        grads = g.gradient(l, params)\n","        grads = grad_clipping(grads, 1)\n","        updater.apply_gradients(zip(grads, params))\n","\n","        # Keras loss by default returns the average loss in a batch\n","        # l_sum = l * float(tf.size(y).numpy()) if isinstance(\n","        #     loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n","        metric.add(l * tf.size(y).numpy(), tf.size(y).numpy())\n","    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Aa8MJdCPkp4u"},"source":["#@save\n","def train_ch8(net, train_iter, vocab, lr, num_epochs, strategy,\n","              use_random_iter=False):\n","    \"\"\"Train a model (defined in Chapter 8).\"\"\"\n","    with strategy.scope():\n","        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","        updater = tf.keras.optimizers.SGD(lr)\n","    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n","                            legend=['train'], xlim=[10, num_epochs])\n","    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab)\n","    # Train and predict\n","    for epoch in range(num_epochs):\n","        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater,\n","                                     use_random_iter)\n","        if (epoch + 1) % 10 == 0:\n","            print(predict('time traveller'))\n","            animator.add(epoch + 1, [ppl])\n","    device = d2l.try_gpu()._device_name\n","    print(f'perplexity {ppl:.1f}, {speed:.1f} tokens/sec on {str(device)}')\n","    print(predict('time traveller'))\n","    print(predict('traveller'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CuIxxC1RktIi"},"source":["num_epochs, lr = 500, 1\n","train_ch8(net, train_iter, vocab, lr, num_epochs, strategy)"],"execution_count":null,"outputs":[]}]}